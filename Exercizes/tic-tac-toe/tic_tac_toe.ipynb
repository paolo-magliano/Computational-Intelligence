{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from random import choice, choices\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-Tac-Toe game\n",
    "The game is implemented as a sum of 15 game, so the goal is pick three number from 1 to 9 that sum to 15. If the numbers are displayed in a 3x3 grid as below the goal is to pick three numbers that are in a straight line (horizontal, vertical or diagonal), as tic tac toe game.\n",
    "\n",
    "| **2** | **7** | **6** |\n",
    "|-------|-------|-------|\n",
    "| **9** | **5** | **1** |\n",
    "| **4** | **3** | **8** |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE = [2, 7, 6, 9, 5, 1, 4, 3, 8]\n",
    "\n",
    "class TicTacToe():\n",
    "    def  __init__(self, board=None, x=None, o=None):\n",
    "        self.board = frozenset(board) if type(board) == set else frozenset(SEQUENCE) \n",
    "        self.x = frozenset(x) if x else frozenset()\n",
    "        self.o = frozenset(o) if o else frozenset()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.board) + \" \" + str(self.x) + \" \" + str(self.o)\n",
    "    \n",
    "    def __key(self):\n",
    "        return (self.board, self.x, self.o)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__key())\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, TicTacToe):\n",
    "            return self.board == other.board and self.x == other.x and self.o == other.o\n",
    "        return NotImplemented\n",
    "    \n",
    "    \"\"\"\n",
    "    Show the board in a human readable format\n",
    "    \"\"\"\n",
    "    def show (self):\n",
    "        for i, move in enumerate(SEQUENCE):\n",
    "            print(\" \", end=\"\")\n",
    "            if move in self.x:\n",
    "                print(\"X\", end=\"\")\n",
    "            elif move in self.o:\n",
    "                print(\"O\", end=\"\")\n",
    "            else:\n",
    "                print(\".\", end=\"\")\n",
    "            if i % 3 == 2:\n",
    "                print()\n",
    "        print()\n",
    "\n",
    "    \"\"\"\n",
    "    Change the board using a possible moves for a player\n",
    "    \"\"\"\n",
    "    def move(self, player, pos):\n",
    "        if pos in self.board:  \n",
    "            if player == 0:\n",
    "                self.x = self.x.union({pos})\n",
    "            else:\n",
    "                self.o = self.o.union({pos})\n",
    "            self.board = self.board.difference({pos})\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if a player has won or if the game is a draw\n",
    "    \"\"\"\n",
    "    def check_win(self):\n",
    "        if TicTacToe.check(self.x):\n",
    "            if TicTacToe.check(self.o):\n",
    "                raise ValueError(\"Both players have won\")\n",
    "            return 0\n",
    "        elif TicTacToe.check(self.o):\n",
    "            return 1\n",
    "        elif len(self.board) == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def copy(self):\n",
    "        return TicTacToe(set(self.board), set(self.x), set(self.o))\n",
    "\n",
    "    \"\"\"\n",
    "    Check all the possible triplets of moves to see if a player has won\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def check(moves):\n",
    "        if any([sum(triple) == 15 for triple in combinations(moves, 3)]):\n",
    "            return True \n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \"\"\"\n",
    "    Flip the board along the diagonal\n",
    "    \"\"\"\n",
    "    def flip_function (x):\n",
    "        if x > 6:\n",
    "            return x - 6\n",
    "        elif x > 3:\n",
    "            return x\n",
    "        else:\n",
    "            return x + 6\n",
    "        \n",
    "    \"\"\"\n",
    "    Rotate the board 90 degrees clockwise\n",
    "    \"\"\"\n",
    "    def rotate_function (x):\n",
    "        return (10 - 2*x) % 10 if x % 2 == 0 else (5 - 2*x) % 10\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform the board using a function above\n",
    "    \"\"\"\n",
    "    def transform(self, function):\n",
    "        return TicTacToe(set([function(i) for i in self.board]), set([function(i) for i in self.x]), set([function(i) for i in self.o]))\n",
    "\n",
    "    \"\"\"\n",
    "    Get the inverse transformations of a list of transformations\n",
    "    \"\"\"\n",
    "    def get_inverse_transformations(transformations):\n",
    "        inverse_transformations = []\n",
    "        for transformation in transformations:\n",
    "            if transformation == TicTacToe.flip_function:\n",
    "                inverse_transformations.append(TicTacToe.flip_function)\n",
    "            else:\n",
    "                for _ in range(3):\n",
    "                    inverse_transformations.append(TicTacToe.rotate_function)\n",
    "        inverse_transformations.reverse()\n",
    "        return inverse_transformations\n",
    "\n",
    "    \"\"\"\n",
    "    Get the transformation of the board to another board\n",
    "    \"\"\"\n",
    "    def get_transformation(self, other):\n",
    "        equilvalent_game = other\n",
    "        transformation = []\n",
    "        for _ in range(2):\n",
    "            for _ in range(4):\n",
    "                if self == equilvalent_game:\n",
    "                    return transformation\n",
    "                equilvalent_game = equilvalent_game.transform(TicTacToe.rotate_function)\n",
    "                transformation.append(TicTacToe.rotate_function)\n",
    "            equilvalent_game = other.transform(TicTacToe.flip_function)\n",
    "            transformation = [TicTacToe.flip_function]\n",
    "            \n",
    "        return []\n",
    "    \n",
    "    \"\"\"\n",
    "    Check if two boards are equivalent by checking if one is a transformation of the other\n",
    "    \"\"\"\n",
    "    def equivalent(self, other):\n",
    "        return self.get_transformation(other) != []   \n",
    "    \n",
    "    \"\"\"\n",
    "    Get an equivalent board and the transformations to get to it from a list of possible equivalent boards\n",
    "    \"\"\"\n",
    "    def get_equivalent(self, possible_equivalents):\n",
    "        for equivalent in possible_equivalents:\n",
    "            transformations = equivalent.get_transformation(self) \n",
    "            if transformations != []:\n",
    "                return equivalent, transformations\n",
    "        return self , []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "The environment gives reward, next state and if the game is finished given the current state and the action. The state is represented as a TicTacToe object, the action is represented as a number from 1 to 9, the reward is a number different for each situation (win, lose, draw, invalid move). The next state is a TicTacToe object, the game is finished if the game is won, lost, draw or if the action is invalid.\n",
    "\n",
    "The environment implements dome strategies to play the game as the oppoent of the agent. The strategies are:\n",
    "* random: pick a random action\n",
    "* win move: if there is a move that wins the game pick it\n",
    "* win loss move: if there is a move that wins the game pick it, otherwise if there is a move that does not let win the agent pick it\n",
    "* me: let the human play\n",
    "\n",
    "The states are saved in a file, so they can be reused in the next run of the program. All the states are not equivalent to each other, so they are minimized using the symmetries of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    INVALID_MOVE_REWARD = -1\n",
    "    MOVE_REWARD = 0.02\n",
    "    WIN_REWARD = 1\n",
    "    LOSE_REWARD = -1\n",
    "    DRAW_REWARD = 0\n",
    "\n",
    "    def __init__(self, player = 0, strategy=None) -> None:\n",
    "        self.states = Environment.get_states()\n",
    "        self.player = player\n",
    "        self.strategy = strategy if strategy else Environment.random_strategy\n",
    "    \n",
    "    \"\"\"\n",
    "    Get all the possible states of the game\n",
    "    \"\"\"\n",
    "    def get_states():\n",
    "        try:\n",
    "            with open('states.npy', 'rb') as f:\n",
    "                states = [np.load(f, allow_pickle=True) for _ in range(len(SEQUENCE) + 1)]\n",
    "        except FileNotFoundError:\n",
    "            states = Environment.generate_states()\n",
    "            with open('states.npy', 'wb') as f:\n",
    "                for state in states:\n",
    "                    np.save(f, state)\n",
    "\n",
    "        return states\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate all the possible states of the game\n",
    "    \"\"\"\n",
    "    def generate_states():\n",
    "        states = []\n",
    "\n",
    "        for depth in range(len(SEQUENCE) + 1):\n",
    "            boards = [set(e) for e in combinations(SEQUENCE, depth)]\n",
    "            good_games = []\n",
    "\n",
    "            for board in boards:\n",
    "                x_and_o = set(SEQUENCE) - board\n",
    "                o = [set(e) for e in combinations(x_and_o, len(x_and_o) // 2)]\n",
    "                x = [x_and_o - x_moves for x_moves in o]\n",
    "                \n",
    "                for x_moves, o_moves in zip(x, o):\n",
    "                    game = TicTacToe(board, x_moves, o_moves)\n",
    "                    equivalent = [game.equivalent(good) for good in good_games]\n",
    "\n",
    "                    if not any(equivalent):\n",
    "                        try:\n",
    "                            game.check_win()\n",
    "                            good_games.append(game)\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                        \n",
    "            states.append(good_games)\n",
    "\n",
    "        states.reverse()\n",
    "\n",
    "        return states\n",
    "    \n",
    "    def random_strategy(self, actions):\n",
    "        return choice(list(actions))\n",
    "    \n",
    "    def win_move_strategy(self, actions):\n",
    "        for action in actions:\n",
    "            game = self.current_state.copy()\n",
    "            game.move(1 - self.player, action)\n",
    "            if game.check_win() == 1 - self.player:\n",
    "                return action\n",
    "        return Environment.random_strategy(self, actions)\n",
    "    \n",
    "    def win_loss_move_strategy(self, actions):\n",
    "        for action in actions:\n",
    "            game = self.current_state.copy()\n",
    "            game.move(1 - self.player, action)\n",
    "            if game.check_win() == 1 - self.player:\n",
    "                return action\n",
    "        for agent_action in actions:\n",
    "            agent_game = game.copy()\n",
    "            agent_game.move(self.player, agent_action)\n",
    "            if agent_game.check_win() == self.player:\n",
    "                return agent_action\n",
    "        return Environment.random_strategy(self, actions)\n",
    "    \n",
    "    def me_strategy(self, actions):\n",
    "        print(\"Current state:\")\n",
    "        show_state = self.transform_inv_state(self.current_state)\n",
    "        show_state.show()\n",
    "        time.sleep(0.2)\n",
    "        index = int(input(\"Enter move: \"))\n",
    "        action = SEQUENCE[index - 1]\n",
    "        for transformation in self.transformations:\n",
    "            action = transformation(action)\n",
    "        return action\n",
    "    \n",
    "    \"\"\"\n",
    "    Reset the environment to initial state\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        self.transformations = []\n",
    "        self.current_state = choice(self.states[0])\n",
    "        if self.player == 1:\n",
    "            action = self.strategy(self, self.current_state.board)\n",
    "            self.current_state.move(1 - self.player, action)\n",
    "\n",
    "            self.update_transformations()\n",
    "\n",
    "        return self.current_state, False\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform the equivalent state back to the original state\n",
    "    \"\"\"\n",
    "    def transform_inv_state(self, state):\n",
    "        for transformation in TicTacToe.get_inverse_transformations(self.transformations):\n",
    "            state = state.transform(transformation)\n",
    "        return state\n",
    "    \n",
    "    \"\"\"\n",
    "    Transform the equivalent action back to the original action\n",
    "    \"\"\"\n",
    "    def transform_inv_action(self, action):\n",
    "        for transformation in TicTacToe.get_inverse_transformations(self.transformations):\n",
    "            action = transformation(action)\n",
    "        return action\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the current state to an equivalent state that is know from the environment\n",
    "    \"\"\"\n",
    "    def update_transformations(self):\n",
    "        possible_equivalents = self.states[len(SEQUENCE) - len(self.current_state.board)]\n",
    "        self.current_state, transformation = self.current_state.get_equivalent(possible_equivalents)\n",
    "        self.transformations += transformation \n",
    "    \n",
    "    \"\"\"\n",
    "    Make a move in the environment\n",
    "    \"\"\"\n",
    "    def step(self, action):\n",
    "        \n",
    "        if not self.current_state.move(self.player, action):\n",
    "            return self.current_state, Environment.INVALID_MOVE_REWARD, True\n",
    "\n",
    "        self.update_transformations()\n",
    "\n",
    "        win = self.current_state.check_win()\n",
    "\n",
    "        if win == self.player:\n",
    "            return self.current_state, Environment.MOVE_REWARD + Environment.WIN_REWARD, True\n",
    "        elif win == -1:\n",
    "            return self.current_state, Environment.MOVE_REWARD + Environment.DRAW_REWARD, True\n",
    "        \n",
    "        env_action = self.strategy(self, self.current_state.board)\n",
    "        self.current_state.move(1 - self.player, env_action)\n",
    "\n",
    "        self.update_transformations()\n",
    "\n",
    "        win = self.current_state.check_win()\n",
    "\n",
    "        if win == 1 - self.player:\n",
    "            return self.current_state, Environment.MOVE_REWARD + Environment.LOSE_REWARD, True\n",
    "        elif win == -1:\n",
    "            return self.current_state, Environment.MOVE_REWARD + Environment.DRAW_REWARD, True\n",
    "        \n",
    "        return self.current_state, Environment.MOVE_REWARD, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The agent is a Q-learning agent that use a Monte Carlo aproach, so from each game it updates the Q value function with the rewards of the environment. The Q values are an estimation of thw expected reward of each action in each state. The Q values are updated using the formula:\n",
    "\n",
    "`Q(s, a) = Q(s, a) + (reward - Q(s, a)) / N(s, a)`\n",
    "\n",
    "where `N(s, a)` is the number of times the agent has visited the state `s` and has taken the action `a`. The reward is the reward of the environment.\n",
    "\n",
    "The agent has a policy that is epsilon greedy, so it picks a random action with probability `epsilon/number of moves` and the best action with probability `epsilon/number of moves + (1 - epsilon)`, where the best action is the action with the highest Q value. The epsilon is decreased at each game, so the agent starts with a random policy and then it starts to exploit the Q values.\n",
    "\n",
    "The Q values is saved in a file, so it can be reused in the next run of the program. ALso the number of games played is saved in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    Good value for greedy_exp:\n",
    "    - 0.1 or 0.2 for promote exploration\n",
    "    - 0.5 for balance exploration and exploitation\n",
    "    - 1 for promote exploitation\n",
    "    - 10 for optimal policy\n",
    "    \"\"\"\n",
    "    def __init__(self, greedy_exp=0.5) -> None:\n",
    "        self.q_values = Agent.get_q_values()\n",
    "        self.episodes = Agent.get_episodes()\n",
    "        self.greedy_exp = greedy_exp\n",
    "\n",
    "    \"\"\"\n",
    "    Define the policy of the agent\n",
    "    \"\"\"\n",
    "    def e_greedy_policy(self, state):\n",
    "        probability = [1/(len(SEQUENCE)*self.episodes**self.greedy_exp) + (1 - 1/(self.episodes**self.greedy_exp)) if action == np.argmax(self.q_values[state][0]) else 1/(len(SEQUENCE)*self.episodes**self.greedy_exp) for action in range(len(SEQUENCE))]\n",
    "        action = choices(SEQUENCE, probability, k=1).pop()\n",
    "\n",
    "        return action\n",
    "\n",
    "    def optimal_policy(self, state):\n",
    "        return SEQUENCE[np.argmax(self.q_values[state][0])]\n",
    "\n",
    "    \"\"\"\n",
    "    Get the q values from a file or generate them\n",
    "    \"\"\"\n",
    "    def get_q_values():\n",
    "        try:\n",
    "            with open('q_values.pkl', 'rb') as fp:\n",
    "                q_values = pickle.load(fp)\n",
    "        except FileNotFoundError:\n",
    "            q_values = Agent.generate_q_values()\n",
    "\n",
    "        return q_values\n",
    "\n",
    "    \"\"\"\n",
    "    Generate the q values\n",
    "    \"\"\"\n",
    "    def generate_q_values():\n",
    "        q_values = {state: np.zeros((2, len(SEQUENCE))) for state in np.concatenate(Environment.get_states())}\n",
    "\n",
    "        return q_values\n",
    "    \"\"\"\n",
    "    Get the number of episodes from a file or generate them\n",
    "    \"\"\"\n",
    "    def get_episodes():\n",
    "        try:\n",
    "            with open('episodes.pkl', 'rb') as fp:\n",
    "                episodes = pickle.load(fp)\n",
    "        except FileNotFoundError:\n",
    "            episodes = 1\n",
    "\n",
    "        return episodes\n",
    "    \"\"\"\n",
    "    Update the q values using the policy improvment algorithm\n",
    "    \"\"\"\n",
    "    def policy_improvment(self, episode_states, episode_actions, episode_rewards):\n",
    "        for index, (state, action) in enumerate(zip(episode_states, episode_actions)):\n",
    "            index_action = SEQUENCE.index(action)\n",
    "            # print(\"State: \", state)\n",
    "            # state.show()\n",
    "            # print(\"Action: \", action)\n",
    "\n",
    "            cumulative_reward = sum(episode_rewards[index:])\n",
    "\n",
    "            # print(\"Cumulative reward: \", cumulative_reward)\n",
    "            # print(\"Q value: \", self.q_values[state][0])\n",
    "            # print(\"Q value count: \", self.q_values[state][1])\n",
    "\n",
    "            self.q_values[state][1][index_action] += 1\n",
    "            self.q_values[state][0][index_action] += (cumulative_reward - self.q_values[state][0][index_action]) / self.q_values[state][1][index_action]\n",
    "            \n",
    "            # print(\"Q value update: \", self.q_values[state][0])\n",
    "            # print(\"Q value count update: \", self.q_values[state][1])\n",
    "        self.episodes += 1\n",
    "        \n",
    "    \"\"\"\n",
    "    Save the q values and the number of episodes to files\n",
    "    \"\"\"\n",
    "    def save(self):\n",
    "        with open('q_values.pkl', 'wb') as fp:\n",
    "            pickle.dump(self.q_values, fp)\n",
    "        with open('episodes.pkl', 'wb') as fp:\n",
    "            pickle.dump(self.episodes, fp)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL algorithm\n",
    "\n",
    "The reinforcement learning algorithm is implemented as a function that run an episode of the game. The function takes as input the agent and the strategy of the opponent. The function returns three list of rewards, states and actions. \n",
    "Each episode is runned until the game is finished. At each step the agent picks an action using the policy, then the environment gives the reward, the next state and if the game is finished. At the end of the episode the Q values are updated using the rewards, states and actions.\n",
    "\n",
    "If you want to train from zero delete the files `q_values.pkl` and `episodes.pkl` and run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1637,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode(agent, player, env_strategy=Environment.random_strategy, verbose=True): \n",
    "    env = Environment(player, env_strategy)\n",
    "    state, end = env.reset()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Agent play as player \", player)\n",
    "\n",
    "    states = [state.copy()]\n",
    "    actions = []\n",
    "    rewards = []\n",
    "\n",
    "    while not end:\n",
    "        if verbose:\n",
    "            value = agent.q_values[state]\n",
    "            show_state = env.transform_inv_state(state)\n",
    "            show_state.show()\n",
    "            print(\"Moves:\\t\\t\\t\", end=\"\")\n",
    "            for i in SEQUENCE:\n",
    "                print(\"{}   \\t\".format(env.transform_inv_action(i)), end=\"\")\n",
    "            print(\"\\nExpected reward:\\t\", end=\"\")\n",
    "            for j in value[0]:\n",
    "                print(\"{:.2f}\\t\".format(j), end=\"\")\n",
    "            print()\n",
    "                \n",
    "        action = agent.optimal_policy(state)\n",
    "        if verbose:\n",
    "            print(\"Agent action: \", env.transform_inv_action(action))\n",
    "            print()\n",
    "        state, reward, end = env.step(action)\n",
    "\n",
    "\n",
    "        states.append(state.copy())\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Final state: \")\n",
    "        show_state = env.transform_inv_state(state)\n",
    "        show_state.show()\n",
    "\n",
    "    return states, actions, rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploaration rate: 0.0000\n",
      "Game won:  709                                                   \n",
      "Game lost:  34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iterations = 1000\n",
    "agent = Agent(0.2)\n",
    "win = 0\n",
    "loss = 0\n",
    "\n",
    "print(f\"Exploaration rate: {1/(agent.episodes**agent.greedy_exp):.4f}\") \n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    states, actions, rewards = episode(agent, i%2, env_strategy=Environment.win_loss_move_strategy, verbose=False)\n",
    "    print(f\"Game: {i} Reward: {sum(rewards):.2f}\", end=\"\\r\") \n",
    "    if sum(rewards) > 0.5:\n",
    "        win += 1\n",
    "    elif sum(rewards) < -0.5:\n",
    "        loss += 1\n",
    "\n",
    "    agent.policy_improvment(states, actions, rewards)\n",
    "\n",
    "agent.save()\n",
    "\n",
    "print(\"Game won: \", win, \" \"*50)  \n",
    "print(\"Game lost: \", loss)\n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play against the agent\n",
    "Set the variable `play_against_agent` to `True` to play against the agent and set the variable `agent_start` to `True` if the agent starts the game.\n",
    "Your possible action are number from 1 to 9 where each number correspond to a cell in the grid as below:\n",
    "\n",
    "| **1** | **2** | **3** |\n",
    "|-------|-------|-------|\n",
    "| **4** | **5** | **6** |\n",
    "| **7** | **8** | **9** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1639,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_against_agent = False\n",
    "agent_start = False\n",
    "\n",
    "if play_against_agent:\n",
    "    agent = Agent(100)\n",
    "    states, actions, rewards = episode(agent, 0 if agent_start else 1, verbose=True, env_strategy=Environment.me_strategy)\n",
    "    if sum(rewards) < -0.5:\n",
    "        print(\"You win!!!\")\n",
    "    elif sum(rewards) > 0.5:\n",
    "        print(\"You lose :(\")\n",
    "    else:\n",
    "        print(\"Draw\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
